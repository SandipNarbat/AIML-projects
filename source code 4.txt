
# code1.py

from pathlib import Path
from datetime import date, datetime, timedelta
import json
import threading
import time
import logging
import logging.handlers
from typing import Dict, Any, Optional, Tuple, List, Callable
from dataclasses import dataclass, asdict
from enum import Enum
import numpy as np
import xgboost as xgb

from preprocessing import preprocess_stream_line
from live_streamer import AutosysLogStreamer, _iter_entries
import code2

# ============================================================================
# CONFIGURATION
# ============================================================================

CONFIG = {
    # Paths
    "baseline_path": "baseline_stats.json",
    "model_dir": "models",
    "log_dir": "logs",
    "dlq_dir": "dlq",
    "training_data_path": "processed",
    
    "log_file_path": r"D:\AUTOSYSLOG\test\event_demon.SBI.09012025.txt",
    "streaming_speed": 1.0,

    "retrain_interval_hours": 24,
    "retrain_sample_size": 1000,
    "min_events_for_retrain": 100,

    "rmse_threshold": 500.0,
    "drift_threshold": 0.15,
    
    "baseline_exceeded_threshold_seconds": 60,
    "baseline_exceeded_threshold_percent": 0.20,
    "alert_on_baseline_exceeded": True,
    "alert_cooldown_seconds": 300,
    "baseline_threshold_mode": "both",
    
    "baseline_method": "statistical",
    "baseline_stdev_multiplier": 2.0,
}

# ============================================================================
# DEAD LETTER QUEUE
# ============================================================================

class DeadLetterQueue:
    """Persists failed records for replay and investigation"""
    
    def __init__(self, dlq_dir: str):
        self.dlq_dir = Path(dlq_dir)
        self.dlq_dir.mkdir(parents=True, exist_ok=True)
        self.dlq_file = self.dlq_dir / f"dlq_{datetime.now().strftime('%Y%m%d')}.txt"
        self.lock = threading.Lock()
        self.added_count = 0
    
    def add(self, line: str, error: str, context: Dict = None):
        """Add failed record to DLQ"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        context_str = str(context) if context else ""
        
        entry = f"[{timestamp}] ERROR: {error}\n"
        entry += f"  Line: {line[:500]}...\n"
        if context_str:
            entry += f"  Context: {context_str}\n"
        entry += "-" * 80 + "\n"
        
        with self.lock:
            try:
                with open(self.dlq_file, 'a', encoding='utf-8') as f:
                    f.write(entry)
                self.added_count += 1
            except Exception as e:
                print(f"CRITICAL: Failed to write DLQ: {str(e)}")
    
    def get_stats(self) -> Dict[str, Any]:
        """Get DLQ statistics"""
        return {
            'dlq_file': str(self.dlq_file),
            'records_added': self.added_count,
            'file_exists': self.dlq_file.exists()
        }

# ============================================================================
# LOGGING - TEXT FORMAT
# ============================================================================

@dataclass
class LogEvent:
    timestamp: str
    level: str
    module: str
    message: str
    context: Dict[str, Any] = None
    
    def to_dict(self):
        return asdict(self)

class StructuredLogger:
    """âœ… Simplified text logging - no JSON serialization needed"""
    
    def __init__(self, log_dir: str):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # File paths - using .txt format
        self.general_log = self.log_dir / "general.log"
        self.events_log = self.log_dir / "events.txt"
        self.retraining_log = self.log_dir / "retraining.txt"
        self.performance_log = self.log_dir / "performance.txt"
        
        # Create rotated file handler
        self.file_handler = logging.handlers.RotatingFileHandler(
            self.general_log, 
            maxBytes=50*1024*1024,
            backupCount=5
        )
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.file_handler.setFormatter(formatter)
        self.lock = threading.Lock()
    
    def _format_value(self, value: Any) -> str:
        """âœ… Direct string conversion - no JSON needed"""
        if isinstance(value, (datetime, date)):
            return value.isoformat()
        elif isinstance(value, float):
            return f"{value:.2f}"
        elif isinstance(value, (np.integer, np.int64, np.int32)):
            return str(int(value))
        elif isinstance(value, (np.floating, np.float64, np.float32)):
            return f"{float(value):.2f}"
        elif isinstance(value, np.ndarray):
            return str(value.tolist())
        else:
            return str(value)
    
    def _format_context(self, context: Dict[str, Any]) -> str:
        """Format context as key=value pairs for text output"""
        if not context:
            return ""
        
        items = []
        for key, value in context.items():
            formatted_value = self._format_value(value)
            items.append(f"{key}={formatted_value}")
        
        return " | ".join(items)
    
    def log_event(self, level: str, module: str, message: str, context: Dict[str, Any] = None):
        """Log event to text file"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        context_str = self._format_context(context) if context else ""
        
        log_line = f"[{timestamp}] {level.ljust(7)} [{module.ljust(20)}] {message}"
        if context_str:
            log_line += f" | {context_str}"
        
        with self.lock:
            try:
                with open(self.events_log, 'a', encoding='utf-8') as f:
                    f.write(log_line + '\n')
            except Exception as e:
                print(f"ERROR: Failed to write log: {e}")
    
    def log_retraining(self, status: str, metrics: Dict[str, Any]):
        """Log retraining event"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        metrics_str = []
        for key, value in metrics.items():
            formatted = self._format_value(value)
            metrics_str.append(f"  {key.ljust(25)}: {formatted}")
        
        log_line = f"[{timestamp}] RETRAINING {status.upper()}\n"
        log_line += "\n".join(metrics_str)
        
        with self.lock:
            try:
                with open(self.retraining_log, 'a', encoding='utf-8') as f:
                    f.write(log_line + '\n' + '='*80 + '\n')
            except Exception as e:
                print(f"ERROR: Failed to write retraining log: {e}")
    
    def log_performance(self, metrics: Dict[str, Any]):
        """Log performance metrics"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        metric_parts = [f"{k}={self._format_value(v)}" for k, v in metrics.items()]
        log_line = f"[{timestamp}] " + " | ".join(metric_parts)
        
        with self.lock:
            try:
                with open(self.performance_log, 'a', encoding='utf-8') as f:
                    f.write(log_line + '\n')
            except Exception as e:
                print(f"ERROR: Failed to write performance log: {e}")
    
    def info(self, module: str, message: str, context: Dict = None):
        self.log_event('INFO', module, message, context)
    
    def warning(self, module: str, message: str, context: Dict = None):
        self.log_event('WARNING', module, message, context)
    
    def error(self, module: str, message: str, context: Dict = None):
        self.log_event('ERROR', module, message, context)
    
    def debug(self, module: str, message: str, context: Dict = None):
        self.log_event('DEBUG', module, message, context)


# Initialize components
logger = StructuredLogger(CONFIG["log_dir"])
dlq = DeadLetterQueue(CONFIG.get("dlq_dir", "dlq"))

# ============================================================================
# MODEL MANAGEMENT
# ============================================================================

class ModelVersion:
    """âœ… Model versioning with validation"""
    
    def __init__(self, model_id: str, booster: xgb.Booster, meta: Dict, 
                 rmse: float, created_at: datetime):
        self.model_id = model_id
        self.booster = booster
        self.meta = meta
        self.rmse = rmse
        self.created_at = created_at
        self.predictions_count = 0
        self.alerts_triggered = 0
    
    def save(self, model_dir: str):
        """Save model and metadata"""
        model_dir = Path(model_dir)
        model_dir.mkdir(parents=True, exist_ok=True)
        
        model_path = model_dir / f"{self.model_id}_model.json"
        meta_path = model_dir / f"{self.model_id}_meta.json"
        
        self.booster.save_model(str(model_path))
        
        metadata = {
            **self.meta,
            'rmse': float(self.rmse),
            'created_at': self.created_at.isoformat(),
        }
        
        with open(meta_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        return model_path, meta_path
    
    @staticmethod
    def load(model_dir: str, model_id: str):
        """âœ… Load model with validation"""
        model_dir = Path(model_dir)
        
        model_path = model_dir / f"{model_id}_model.json"
        meta_path = model_dir / f"{model_id}_meta.json"
        
        if not model_path.exists() or not meta_path.exists():
            return None
        
        try:
            booster = xgb.Booster()
            booster.load_model(str(model_path))
            
            with open(meta_path, 'r') as f:
                meta = json.load(f)
            
            rmse = meta.pop('rmse', None)
            created_at_str = meta.pop('created_at', datetime.now().isoformat())
            created_at = datetime.fromisoformat(created_at_str)
            
            loaded_features = meta.get('feature_columns', [])
            if loaded_features != code2.FEATURE_COLUMNS:
                logger.warning('ModelVersion', 
                             f'Feature mismatch when loading {model_id}',
                             {'expected': code2.FEATURE_COLUMNS,
                              'got': loaded_features})
            
            return ModelVersion(model_id, booster, meta, rmse, created_at)
        
        except Exception as e:
            logger.error('ModelVersion', f'Failed to load {model_id}: {str(e)}')
            return None

class ModelManager:
    """Manages active model, versioning, and A/B testing"""
    
    def __init__(self, model_dir: str):
        self.model_dir = model_dir
        self.active_model: Optional[ModelVersion] = None
        self.candidate_model: Optional[ModelVersion] = None
        self.lock = threading.Lock()
        self.model_registry = {}
    
    def set_active_model(self, model: ModelVersion):
        """Set as active production model"""
        with self.lock:
            self.active_model = model
            self.model_registry[model.model_id] = model
            logger.info('ModelManager', f'Model {model.model_id} set as active', 
                       {'rmse': model.rmse})
    
    def set_candidate_model(self, model: ModelVersion):
        """Set candidate for evaluation"""
        with self.lock:
            self.candidate_model = model
            self.model_registry[model.model_id] = model
    
    def promote_candidate(self) -> bool:
        """Promote candidate to active if better than current"""
        with self.lock:
            if not self.candidate_model:
                return False
            
            if not self.active_model or self.candidate_model.rmse < self.active_model.rmse:
                old_active = self.active_model
                self.active_model = self.candidate_model
                self.candidate_model = None
                
                logger.info('ModelManager', 'Candidate promoted to active',
                           {'old_model': old_active.model_id if old_active else None,
                            'new_model': self.active_model.model_id,
                            'old_rmse': old_active.rmse if old_active else None,
                            'new_rmse': self.active_model.rmse})
                return True
            
            return False
    
    def predict(self, job_name: str, joid: int, baselines: Dict[str, Dict[str, Any]]) -> Optional[float]:
        """âœ… Make prediction with performance logging"""
        with self.lock:
            if not self.active_model:
                logger.warning('ModelManager', 'No active model available')
                return None
            
            try:
                key = f"{job_name}|{joid}"
                baseline = baselines.get(key)
                
                if not baseline:
                    logger.warning('ModelManager', f'No baseline found for {key}')
                    return None
                
                feature_columns = self.active_model.meta.get('feature_columns', [])
                values = []
                for col in feature_columns:
                    col_name = col.split("baseline_")[-1]
                    values.append(float(baseline.get(col_name, baseline.get(col, 0.0))))
                
                x = np.asarray([values], dtype=float)
                dmatrix = xgb.DMatrix(x, feature_names=feature_columns)
                pred = float(self.active_model.booster.predict(dmatrix)[0])
                
                self.active_model.predictions_count += 1
                
                logger.log_performance({
                    'type': 'prediction',
                    'job_name': job_name,
                    'joid': joid,
                    'predicted_duration': pred,
                    'model_id': self.active_model.model_id,
                    'prediction_number': self.active_model.predictions_count
                })
                
                return pred
            
            except Exception as e:
                logger.error('ModelManager', f'Prediction failed: {str(e)}', 
                           {'job_name': job_name, 'joid': joid})
                return None

# ============================================================================
# STATE MANAGEMENT - âœ… CRITICAL FIX: Use Log Timestamps
# ============================================================================

@dataclass
class JobInstance:
    """âœ… Tracks a running job instance with log-based timestamps"""

    joid: str
    runid: str
    job_name: str

    # âœ… CRITICAL FIX: Store log timestamp for accurate duration calculation
    start_time_log: datetime         # Timestamp from AutoSys log (ts field)
    start_time_real: datetime        # Real-time clock (when START log received)

    predicted_duration: Optional[float]
    baseline_seconds: Optional[float]

    alerted: bool = False
    last_alert_time: Optional[datetime] = None

    def duration_so_far_real(self) -> float:
        """Real-time duration (for immediate alerting)"""
        return (datetime.now() - self.start_time_real).total_seconds()

    def true_duration_from_log(self, end_time_log: datetime) -> float:
        """âœ… CRITICAL FIX: Calculate TRUE duration using log timestamps only"""
        return (end_time_log - self.start_time_log).total_seconds()

    def exceeded_baseline(self, threshold_seconds: float, threshold_percent: float, mode: str = "either") -> bool:
        """Uses REAL-TIME duration to detect overrun immediately"""
        threshold = self.predicted_duration or self.baseline_seconds
        if not threshold or threshold <= 0:
            return False

        current_duration = self.duration_so_far_real()

        if current_duration <= threshold:
            return False

        exceeded_by_seconds = current_duration - threshold
        exceeded_by_percent = exceeded_by_seconds / threshold

        if mode == "both":
            return (exceeded_by_seconds >= threshold_seconds and exceeded_by_percent >= threshold_percent)
        else:
            return (exceeded_by_seconds >= threshold_seconds or exceeded_by_percent >= threshold_percent)

    def get_exceeded_amount(self) -> dict:
        """Amount exceeded (REAL TIME)"""
        current_duration = self.duration_so_far_real()
        if not self.predicted_duration:
            return {"seconds": 0, "percent": 0}

        exceeded_seconds = max(0, current_duration - self.predicted_duration)
        exceeded_percent = exceeded_seconds / self.predicted_duration if self.predicted_duration > 0 else 0

        return {
            "seconds": round(exceeded_seconds, 2),
            "percent": round(exceeded_percent * 100, 2)
        }

class StateManager:
    """Manages running job state"""
    
    def __init__(self, alert_cooldown: int = 300):
        self.running_jobs: Dict[Tuple[str, str], JobInstance] = {}
        self.lock = threading.Lock()
        self.alert_cooldown = alert_cooldown
    
    def start_job(self, joid: str, runid: str, job_name: str,
                  predicted_duration: float, baseline_seconds: float,
                  log_ts: datetime):
        """âœ… Start tracking job with log timestamp for accuracy"""

        key = (runid, joid)

        with self.lock:
            self.running_jobs[key] = JobInstance(
                joid=str(joid),
                runid=str(runid),
                job_name=str(job_name),

                start_time_log=log_ts,           # âœ… CRITICAL: Use log timestamp
                start_time_real=datetime.now(),  # Real-time for alerting

                predicted_duration=float(predicted_duration or 0),
                baseline_seconds=float(baseline_seconds or 0),
            )
    
    def end_job(self, joid: str, runid: str) -> Optional[JobInstance]:
        """Remove job and return its instance"""
        key = (runid, joid)
        with self.lock:
            return self.running_jobs.pop(key, None)
    
    def get_active_jobs(self) -> list:
        """Get snapshot of active jobs"""
        with self.lock:
            return list(self.running_jobs.values())
    
    def can_alert_for_job(self, key: Tuple[str, str]) -> bool:
        """Check if job can be alerted (respects cooldown)"""
        with self.lock:
            job = self.running_jobs.get(key)
            if not job:
                return False
            
            if job.alerted and job.last_alert_time:
                if (datetime.now() - job.last_alert_time).total_seconds() < self.alert_cooldown:
                    return False
            
            job.alerted = True
            job.last_alert_time = datetime.now()
            return True

# Global state
model_manager = ModelManager(CONFIG["model_dir"])
state_manager = StateManager(alert_cooldown=CONFIG["alert_cooldown_seconds"])
event_buffer = []
buffer_lock = threading.Lock()

# ============================================================================
# MAIN PROCESSING - âœ… CRITICAL FIX: Use Log Timestamps
# ============================================================================

def handle_stream_output(line_text: str):
    """âœ… CRITICAL FIX: Calculate duration using log timestamps, not system clock"""
    try:
        rec = preprocess_stream_line(line_text)
        if not rec:
            return
        
        # Validate required fields
        required_fields = ["JOID", "RUNID", "JobName", "Status", "timestamp"]
        missing = [f for f in required_fields if f not in rec]
        if missing:
            logger.warning('StreamHandler', 'Missing required fields', 
                         {'missing': missing})
            dlq.add(line_text, 'Missing required fields', {'missing': missing})
            return
        
        # Type conversion and validation
        try:
            joid = int(rec["JOID"]) if rec["JOID"] else None
            runid = int(rec["RUNID"]) if rec["RUNID"] else None
            job_name = str(rec["JobName"]).strip()
            status = str(rec["Status"]).strip().upper()
            ts = rec["timestamp"]  # âœ… CRITICAL: This is the log timestamp from "ts" field
        except (ValueError, TypeError) as e:
            logger.warning('StreamHandler', f'Type conversion error: {str(e)}')
            dlq.add(line_text, f'Type conversion error: {str(e)}', {'record': rec})
            return
        
        ts_str = ts.isoformat() if ts else None
        baselines = load_baselines(CONFIG["baseline_path"])
        
        if status == "STARTING":
            try:
                pred_duration = model_manager.predict(job_name, joid, baselines)
                if pred_duration is None:
                    pred_duration = 0
                
                # Use statistical baseline (mean + 2*stdev)
                baseline_key = f"{job_name}|{joid}"
                baseline_stats = baselines.get(baseline_key, {})
                baseline_seconds = baseline_stats.get("baseline_statistical") or \
                                 baseline_stats.get("median", 0)
                
                # âœ… CRITICAL FIX: Pass log timestamp for accurate tracking
                state_manager.start_job(
                    joid=str(joid),
                    runid=str(runid),
                    job_name=job_name,
                    predicted_duration=pred_duration,
                    baseline_seconds=baseline_seconds,
                    log_ts=ts  # âœ… CRITICAL: Use log timestamp, NOT datetime.now()
                )
                
                logger.info('StreamHandler', 'Job started',
                           {'ts': ts_str,
                            'joid': joid, 
                            'job_name': job_name, 
                            'pred_duration': round(pred_duration or 0, 2), 
                            'baseline_seconds': round(baseline_seconds, 2),
                            'baseline_method': 'mean+2*stdev',
                            'runid': runid})
            
            except Exception as e:
                logger.error('StreamHandler', f'Error handling STARTING: {str(e)}',
                            {'joid': joid, 'job_name': job_name})
                dlq.add(line_text, str(e), {'status': 'STARTING', 'joid': joid})
        
        elif status == "SUCCESS":
            try:
                job_instance = state_manager.end_job(str(joid), str(runid))
                if job_instance:
                    # âœ… CRITICAL FIX: Calculate duration using log timestamps ONLY
                    success_ts = ts  # Log timestamp from "ts" field
                    actual_duration = job_instance.true_duration_from_log(success_ts)
                    
                    # Atomic buffer operation
                    with buffer_lock:
                        event_buffer.append({
                            'joid': str(joid),
                            'runid': str(runid),
                            'job_name': job_name,
                            'actual_duration': actual_duration,  # âœ… TRUE measurement from log
                            'predicted_duration': job_instance.predicted_duration,
                            'timestamp': success_ts.isoformat()
                        })
                    
                    logger.info('StreamHandler', 'Job succeeded',
                               {'ts': ts_str,
                                'joid': joid, 
                                'job_name': job_name,
                                'actual_duration': round(actual_duration, 2),
                                'predicted_duration': round(job_instance.predicted_duration or 0, 2),
                                'runid': runid})
            except Exception as e:
                logger.error('StreamHandler', f'Error handling SUCCESS: {str(e)}')
                dlq.add(line_text, str(e), {'status': 'SUCCESS'})
        
        elif status == "FAILURE":
            try:
                joid_str = str(joid) if joid else None
                runid_str = str(runid) if runid else None
                
                job_instance = state_manager.end_job(joid_str, runid_str) if joid_str and runid_str else None
                
                if job_instance:
                    # âœ… CRITICAL FIX: Use log timestamp for true duration
                    failure_ts = ts
                    actual_duration = job_instance.true_duration_from_log(failure_ts)
                    
                    with buffer_lock:
                        event_buffer.append({
                            'joid': joid_str,
                            'runid': runid_str,
                            'job_name': str(job_name),
                            'actual_duration': actual_duration,
                            'predicted_duration': job_instance.predicted_duration or 0.0,
                            'status': 'FAILURE',
                            'timestamp': failure_ts.isoformat()
                        })
                    
                    logger.warning('StreamHandler', 'Job failed (tracked)', {
                        'ts': ts_str,
                        'joid': int(joid) if joid else None,
                        'job_name': str(job_name),
                        'duration': round(actual_duration, 2),
                        'predicted_duration': round(job_instance.predicted_duration or 0, 2),
                        'runid': int(runid) if runid else None
                    })
                else:
                    logger.warning('StreamHandler', 'Job failed (untracked)', {
                        'ts': ts_str,
                        'joid': int(joid) if joid else None,
                        'job_name': str(job_name),
                        'runid': int(runid) if runid else None
                    })
            except Exception as e:
                logger.error('StreamHandler', f'Error handling FAILURE: {str(e)}')
                dlq.add(line_text, str(e), {'status': 'FAILURE'})
    
    except Exception as e:
        logger.error('StreamHandler', f'Unexpected error: {str(e)}',
                    {'line_preview': line_text[:200]})
        dlq.add(line_text, str(e), {'type': 'unexpected_error'})

# ============================================================================
# MONITORING & RETRAINING
# ============================================================================

def monitor_baselines():
    """Monitor running jobs and alert on significant baseline overrun"""
    logger.info('BaselineMonitor', 'Baseline monitoring thread started')
    
    threshold_seconds = CONFIG.get("baseline_exceeded_threshold_seconds", 60)
    threshold_percent = CONFIG.get("baseline_exceeded_threshold_percent", 0.20)
    threshold_mode = CONFIG.get("baseline_threshold_mode", "either")
    
    logger.info('BaselineMonitor', 'Alert thresholds configured', {
        'threshold_seconds': threshold_seconds,
        'threshold_percent': f"{threshold_percent * 100}%",
        'mode': threshold_mode
    })
    
    while True:
        try:
            active_jobs = state_manager.get_active_jobs()
            
            for job in active_jobs:
                key = (job.runid, job.joid)
                
                if CONFIG["alert_on_baseline_exceeded"]:
                    if job.exceeded_baseline(
                        threshold_seconds=threshold_seconds,
                        threshold_percent=threshold_percent,
                        mode=threshold_mode
                    ):
                        if state_manager.can_alert_for_job(key):
                            exceeded_info = job.get_exceeded_amount()
                            
                            logger.warning('BaselineMonitor', 'Baseline exceeded', {
                                'joid': job.joid,
                                'runid': job.runid,
                                'job_name': job.job_name,
                                'baseline_seconds': round(job.predicted_duration, 2),
                                'current_duration': round(job.duration_so_far_real(), 2),
                                'exceeded_by': exceeded_info['seconds'],
                                'exceeded_by_percent': f"{exceeded_info['percent']}%"
                            })
                            
                            print(f"âš ï¸ ALERT: {job.job_name} exceeded prediction by {exceeded_info['seconds']}s ({exceeded_info['percent']}%)")
            
            time.sleep(5)
        
        except Exception as e:
            logger.error('BaselineMonitor', f'Monitor error: {str(e)}')
            time.sleep(5)

def retrain_model_periodic():
    """âœ… Atomic buffer operations, metrics tracking"""
    logger.info('Retrainer', 'Model retraining thread started')
    
    last_retrain = datetime.now()
    
    while True:
        try:
            time.sleep(60)
            
            if (datetime.now() - last_retrain).total_seconds() < CONFIG["retrain_interval_hours"] * 3600:
                continue
            
            # Atomic operation - check AND clear together
            with buffer_lock:
                if len(event_buffer) < CONFIG["min_events_for_retrain"]:
                    logger.debug('Retrainer', 
                               f'Waiting for events: {len(event_buffer)}/{CONFIG["min_events_for_retrain"]}')
                    continue
                
                recent_events = event_buffer[-CONFIG["retrain_sample_size"]:]
                events_to_process = list(recent_events)
                event_buffer.clear()
            
            logger.info('Retrainer', f'Starting retraining with {len(events_to_process)} events')
            
            try:
                trained_model, metrics = train_new_model(events_to_process)
                
                if trained_model is None:
                    logger.error('Retrainer', 'Model training failed')
                    continue
                
                model_id = f"model_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                candidate = ModelVersion(
                    model_id=model_id,
                    booster=trained_model,
                    meta={'feature_columns': code2.FEATURE_COLUMNS},
                    rmse=metrics.rmse,
                    created_at=datetime.now()
                )
                
                candidate.save(CONFIG["model_dir"])
                model_manager.set_candidate_model(candidate)
                promoted = model_manager.promote_candidate()
                
                logger.info('Retrainer', 'Retraining completed',
                           {'model_id': model_id, 'rmse': round(metrics.rmse, 2),
                            'promoted': promoted, 'events_used': len(events_to_process)})
                
                logger.log_retraining('success', {
                    'model_id': model_id,
                    'rmse': metrics.rmse,
                    'mae': metrics.mae,
                    'mape': metrics.mape,
                    'r2': metrics.r2,
                    'training_time': metrics.training_time_sec,
                    'promoted': promoted,
                    'events_used': len(events_to_process)
                })
                
                last_retrain = datetime.now()
            
            except Exception as e:
                logger.error('Retrainer', f'Retraining failed: {str(e)}')
                logger.log_retraining('failed', {'error': str(e)})
        
        except Exception as e:
            logger.error('Retrainer', f'Unexpected error: {str(e)}')
            time.sleep(5)

# ============================================================================
# MODEL TRAINING
# ============================================================================

def train_new_model(events: list) -> Tuple[Optional[xgb.Booster], Optional[code2.ModelMetrics]]:
    """âœ… Return metrics object instead of just RMSE"""
    if not events or len(events) < CONFIG["min_events_for_retrain"]:
        return None, None
    
    try:
        X = []
        y = []
        baselines = load_baselines(CONFIG["baseline_path"])
        
        for event in events:
            job_name = event['job_name']
            joid = event['joid']
            actual_duration = event['actual_duration']
            
            key = f"{job_name}|{joid}"
            baseline = baselines.get(key)
            
            if not baseline:
                continue
            
            feature_columns = code2.FEATURE_COLUMNS
            features = []
            for col in feature_columns:
                col_name = col.split("baseline_")[-1]
                features.append(float(baseline.get(col_name, baseline.get(col, 0.0))))
            
            X.append(features)
            y.append(actual_duration)
        
        if len(X) < CONFIG["min_events_for_retrain"]:
            logger.warning('ModelTraining', f'Insufficient training samples: {len(X)}')
            return None, None
        
        X = np.array(X, dtype=float)
        y = np.array(y, dtype=float)
        
        dtrain = xgb.DMatrix(X, label=y)
        
        params = {
            'objective': 'reg:squarederror',
            'max_depth': 5,
            'learning_rate': 0.05,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'n_estimators': 100,
            'tree_method': 'hist',
            'device': 'cpu',
            'nthread': 1  # Single-threaded for thread safety
        }
        
        model = xgb.train(params, dtrain, num_boost_round=params['n_estimators'])
        
        preds = model.predict(dtrain)
        mse = np.mean((preds - y) ** 2)
        rmse = np.sqrt(mse)
        
        metrics = code2.compute_metrics(y, preds, training_time=0.0, model_size=0.0)
        
        return model, metrics
    
    except Exception as e:
        logger.error('ModelTraining', f'Training error: {str(e)}')
        return None, None

def load_baselines(path: str) -> Dict[str, Dict[str, Any]]:
    """Load baseline statistics"""
    try:
        with open(path, "r", encoding="utf-8") as fh:
            return json.load(fh)
    except FileNotFoundError:
        logger.error('Baselines', f'Baseline file not found: {path}')
        return {}
    except json.JSONDecodeError:
        logger.error('Baselines', f'Invalid JSON in baseline file: {path}')
        return {}

# ============================================================================
# CLOCK-SYNCHRONIZED STREAMING
# ============================================================================

def stream_with_clock_sync(
    streamer: AutosysLogStreamer,
    emit: Callable[[str], None],
) -> None:
    """
    âœ… Clock-synchronized streaming that respects log timestamps
    This replays logs maintaining temporal relationships between events
    """
    log_path = streamer.log_path
    speed = streamer.speed

    # Read all entries once (file is finite historic log)
    entries = list(_iter_entries(log_path))
    if not entries:
        logger.warning('Streamer', 'No entries found in log file')
        return

    # Find first entry that has a timestamp
    first_ts: Optional[datetime] = None
    for e in entries:
        if e.timestamp is not None:
            first_ts = e.timestamp
            break

    if first_ts is None:
        # No timestamps in file â†’ just emit without timing control
        logger.warning('Streamer', 'No timestamps found, streaming without clock sync')
        for e in entries:
            if streamer._should_emit(e):
                emit(e.text)
        return

    start_wall = datetime.now()
    logger.info('Streamer', 'Starting clock-synchronized streaming', {
        'first_log_timestamp': first_ts.isoformat(),
        'speed_multiplier': speed,
        'total_entries': len(entries)
    })

    entries_emitted = 0
    for entry in entries:
        if not streamer._should_emit(entry):
            continue

        if entry.timestamp is not None:
            # Offset in original Autosys time
            delta = (entry.timestamp - first_ts).total_seconds()
            # Apply speed factor
            logical_offset = delta / speed
            target_wall = start_wall + timedelta(seconds=logical_offset)

            now = datetime.now()
            wait = (target_wall - now).total_seconds()
            if wait > 0:
                time.sleep(wait)

        # At this point we are as close as possible to the target wall-clock time
        emit(entry.text)
        entries_emitted += 1
        
        # Progress logging every 100 entries
        if entries_emitted % 100 == 0:
            logger.info('Streamer', f'Streamed {entries_emitted}/{len(entries)} entries')
    
    logger.info('Streamer', 'Streaming completed', {
        'entries_emitted': entries_emitted,
        'total_entries': len(entries)
    })

# ============================================================================
# CONFIGURATION VALIDATION
# ============================================================================

def validate_config():
    """âœ… Validate critical configuration values"""
    errors = []
    
    # Type checks
    if not isinstance(CONFIG["streaming_speed"], (int, float)) or CONFIG["streaming_speed"] <= 0:
        errors.append("streaming_speed must be positive number")
    
    if CONFIG["baseline_exceeded_threshold_percent"] < 0 or CONFIG["baseline_exceeded_threshold_percent"] > 1:
        errors.append("baseline_exceeded_threshold_percent must be 0-1")
    
    if CONFIG["baseline_threshold_mode"] not in ["either", "both"]:
        errors.append("baseline_threshold_mode must be 'either' or 'both'")
    
    # File existence checks
    if not Path(CONFIG["baseline_path"]).exists():
        errors.append(f"baseline_path not found: {CONFIG['baseline_path']}")
    
    if not Path(CONFIG["log_file_path"]).exists():
        errors.append(f"log_file_path not found: {CONFIG['log_file_path']}")
    
    if errors:
        for error in errors:
            logger.error('Config', error)
        raise ValueError(f"Configuration invalid: {errors}")
    
    logger.info('Config', 'Configuration validated successfully')

# ============================================================================
# INITIALIZATION & STARTUP
# ============================================================================

def initialize_system():
    """Initialize system and load or create initial model"""
    logger.info('System', 'Initializing anomaly detection system')
    
    Path(CONFIG["model_dir"]).mkdir(parents=True, exist_ok=True)
    
    model_dir = Path(CONFIG["model_dir"])
    models = sorted(model_dir.glob("*_model.json"), reverse=True)
    
    if models:
        latest_model_id = models[0].stem.replace("_model", "")
        model = ModelVersion.load(CONFIG["model_dir"], latest_model_id)
        if model:
            model_manager.set_active_model(model)
            logger.info('System', f'Loaded model {latest_model_id}', 
                       {'rmse': model.rmse})
    
    if not model_manager.active_model:
        logger.warning('System', 'No initial model found')

# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    """Main entry point with clock-synchronized streaming"""
    try:
        logger.info('Main', 'Starting anomaly detection system')
        
        validate_config()
        
        initialize_system()
        
        # Start monitoring thread
        monitor_thread = threading.Thread(target=monitor_baselines, daemon=True)
        monitor_thread.start()
        logger.info('Main', 'Started baseline monitor thread')
        
        # Start retraining thread
        retrain_thread = threading.Thread(target=retrain_model_periodic, daemon=True)
        retrain_thread.start()
        logger.info('Main', 'Started retraining thread')
        
        # Create streamer
        streamer = AutosysLogStreamer(
            log_path=Path(CONFIG["log_file_path"]),
            speed=CONFIG["streaming_speed"]
        )
        
        logger.info('Main', 'Starting clock-synchronized log stream', 
                   {'log_path': CONFIG["log_file_path"],
                    'speed': CONFIG["streaming_speed"],
                    'method': 'clock_sync'})
        
        # âœ… CRITICAL FIX: Use clock-synchronized streaming
        stream_with_clock_sync(streamer, emit=handle_stream_output)
        
        logger.info('Main', 'Streaming completed normally')
    
    except KeyboardInterrupt:
        logger.info('Main', 'System shutdown requested by user')
    except Exception as e:
        logger.error('Main', f'System error: {str(e)}')
        import traceback
        logger.error('Main', traceback.format_exc())
    finally:
        logger.info('Main', 'Anomaly detection system stopped')
        print(f"\nðŸ“Š Final Statistics:")
        print(f"   DLQ Records: {dlq.get_stats()['records_added']}")
        if model_manager.active_model:
            print(f"   Active Model: {model_manager.active_model.model_id}")
            print(f"   Predictions Made: {model_manager.active_model.predictions_count}")
            print(f"   Alerts Triggered: {model_manager.active_model.alerts_triggered}")

if __name__ == "__main__":
    main()
    
    ----------------------------------------
    
    # code2.py

import json
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
import xgboost as xgb
import lightgbm as lgb
from pathlib import Path
import time

# ============================================================================
# âœ… UPDATED FEATURE COLUMNS WITH STATISTICAL BASELINE
# ============================================================================

FEATURE_COLUMNS = [
    "baseline_count",
    "baseline_mean",
    "baseline_stdev",           # âœ… NEW: Variability measure
    "baseline_p95",
    "baseline_mad",
    "baseline_statistical",     # âœ… NEW: Mean + 2*stdev (95.4% confidence)
]

# ============================================================================
# MODEL EVALUATION METRICS
# ============================================================================

@dataclass
class ModelMetrics:
    """âœ… Comprehensive performance metrics for model evaluation"""
    rmse: float
    mae: float
    mape: float  # Mean Absolute Percentage Error
    r2: float
    model_size_mb: float
    training_time_sec: float
    
    def __str__(self):
        return (f"RMSE: {self.rmse:.2f}s | MAE: {self.mae:.2f}s | "
                f"MAPE: {self.mape:.2%} | RÂ²: {self.r2:.4f} | "
                f"Size: {self.model_size_mb:.2f}MB | Time: {self.training_time_sec:.2f}s")
    
    def to_dict(self) -> Dict:
        """Convert metrics to dictionary for logging"""
        return {
            'rmse': float(self.rmse),
            'mae': float(self.mae),
            'mape': float(self.mape),
            'r2': float(self.r2),
            'model_size_mb': float(self.model_size_mb),
            'training_time_sec': float(self.training_time_sec)
        }

def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray, 
                   training_time: float, model_size: float) -> ModelMetrics:
    """
    âœ… Compute comprehensive metrics for model evaluation
    
    Args:
        y_true: Actual values
        y_pred: Predicted values
        training_time: Training time in seconds
        model_size: Model size in MB
    
    Returns:
        ModelMetrics object with all metrics
    """
    mse = np.mean((y_pred - y_true) ** 2)
    rmse = np.sqrt(mse)
    mae = np.mean(np.abs(y_pred - y_true))
    
    # MAPE (avoid division by zero)
    mask = y_true != 0
    if mask.sum() > 0:
        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))
    else:
        mape = 0.0
    
    # RÂ² score
    ss_res = np.sum((y_true - y_pred) ** 2)
    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)
    r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0.0
    
    return ModelMetrics(
        rmse=rmse,
        mae=mae,
        mape=mape,
        r2=r2,
        model_size_mb=model_size,
        training_time_sec=training_time
    )

# ============================================================================
# XGBOOST MODEL TRAINER
# ============================================================================

class XGBoostTrainer:
    """Fast, accurate XGBoost model trainer"""
    
    def __init__(self, feature_columns: List[str]):
        self.feature_columns = feature_columns
    
    def train(self, X: np.ndarray, y: np.ndarray, 
              eval_split: float = 0.2) -> Tuple[xgb.Booster, ModelMetrics]:
        """
        Train XGBoost model and return comprehensive metrics
        
        Args:
            X: Feature matrix
            y: Target values
            eval_split: Fraction of data for evaluation
        
        Returns:
            Tuple of (trained model, metrics)
        """
        start_time = time.time()
        
        # Split data
        n_samples = len(X)
        n_eval = int(n_samples * eval_split)
        
        X_train, X_eval = X[:-n_eval], X[-n_eval:]
        y_train, y_eval = y[:-n_eval], y[-n_eval:]
        
        # Optimized parameters
        params = {
            'objective': 'reg:squarederror',
            'max_depth': 5,
            'learning_rate': 0.05,
            'subsample': 0.9,
            'colsample_bytree': 0.8,
            'min_child_weight': 1,
            'gamma': 0,
            'tree_method': 'hist',
            'device': 'cpu',
        }
        
        dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=self.feature_columns)
        deval = xgb.DMatrix(X_eval, label=y_eval, feature_names=self.feature_columns)
        
        # Early stopping
        evals = [(dtrain, 'train'), (deval, 'eval')]
        evals_result = {}
        
        model = xgb.train(
            params,
            dtrain,
            num_boost_round=200,
            evals=evals,
            evals_result=evals_result,
            early_stopping_rounds=20,
            verbose_eval=False
        )
        
        training_time = time.time() - start_time
        
        # Evaluate on test set
        y_pred_eval = model.predict(deval)
        
        # Get model size
        import sys
        model_size = sys.getsizeof(model) / (1024 * 1024)  # MB
        
        metrics = compute_metrics(y_eval, y_pred_eval, training_time, model_size)
        
        return model, metrics

# ============================================================================
# LIGHTGBM MODEL TRAINER
# ============================================================================

class LightGBMTrainer:
    """Ultra-fast LightGBM for real-time retraining"""
    
    def __init__(self, feature_columns: List[str]):
        self.feature_columns = feature_columns
    
    def train(self, X: np.ndarray, y: np.ndarray,
              eval_split: float = 0.2) -> Tuple[lgb.Booster, ModelMetrics]:
        """
        Train LightGBM and return comprehensive metrics
        
        Args:
            X: Feature matrix
            y: Target values
            eval_split: Fraction of data for evaluation
        
        Returns:
            Tuple of (trained model, metrics)
        """
        start_time = time.time()
        
        # Split data
        n_samples = len(X)
        n_eval = int(n_samples * eval_split)
        
        X_train, X_eval = X[:-n_eval], X[-n_eval:]
        y_train, y_eval = y[:-n_eval], y[-n_eval:]
        
        # LightGBM parameters
        params = {
            'objective': 'regression',
            'metric': 'rmse',
            'max_depth': 5,
            'learning_rate': 0.05,
            'num_leaves': 31,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.9,
            'bagging_freq': 5,
            'verbose': -1,
        }
        
        train_data = lgb.Dataset(X_train, label=y_train, feature_name=self.feature_columns)
        valid_data = lgb.Dataset(X_eval, label=y_eval, feature_name=self.feature_columns, 
                                reference=train_data)
        
        model = lgb.train(
            params,
            train_data,
            num_boost_round=200,
            valid_sets=[train_data, valid_data],
            early_stopping_rounds=20,
            verbose_eval=False
        )
        
        training_time = time.time() - start_time
        
        # Evaluate
        y_pred_eval = model.predict(X_eval)
        
        # Get model size
        import sys
        model_size = sys.getsizeof(model) / (1024 * 1024)  # MB
        
        metrics = compute_metrics(y_eval, y_pred_eval, training_time, model_size)
        
        return model, metrics

# ============================================================================
# MODEL FACTORY
# ============================================================================

class ModelFactory:
    """Factory for creating and training models"""
    
    @staticmethod
    def train_xgboost(X: np.ndarray, y: np.ndarray,
                     feature_columns: List[str]) -> Tuple[xgb.Booster, ModelMetrics]:
        """Train XGBoost and return metrics"""
        trainer = XGBoostTrainer(feature_columns)
        return trainer.train(X, y)
    
    @staticmethod
    def train_lightgbm(X: np.ndarray, y: np.ndarray,
                      feature_columns: List[str]) -> Tuple[lgb.Booster, ModelMetrics]:
        """Train LightGBM and return metrics"""
        trainer = LightGBMTrainer(feature_columns)
        return trainer.train(X, y)

# ============================================================================
# BASELINE STATISTICS GENERATION - âœ… CRITICAL: Mean + 2*StdDev Method
# ============================================================================

def generate_baseline_statistics(job_durations_dict: dict) -> dict:
    """
    âœ… Generate baseline statistics using Mean + 2*StdDev method
    
    This captures 95.4% of normal execution times (assumes normal distribution).
    Any job exceeding mean + 2*stdev is statistically anomalous.
    
    Args:
        job_durations_dict: {"job_name|joid": [duration1, duration2, ...]}
    
    Returns:
        Baselines dict with statistical threshold
    """
    baselines = {}
    
    for key, durations in job_durations_dict.items():
        if not durations or len(durations) < 2:
            baselines[key] = {
                "count": len(durations),
                "mean": durations[0] if durations else 0,
                "median": durations[0] if durations else 0,
                "stdev": 0,
                "p95": durations[0] if durations else 0,
                "mad": 0,
                "baseline_statistical": durations[0] if durations else 0,
                "min": durations[0] if durations else 0,
                "max": durations[0] if durations else 0
            }
            continue
        
        durations_array = np.array(durations, dtype=float)
        
        mean = float(np.mean(durations_array))
        median = float(np.median(durations_array))
        stdev = float(np.std(durations_array, ddof=1))
        p95 = float(np.percentile(durations_array, 95))
        mad = float(np.mean(np.abs(durations_array - mean)))
        
        # âœ… CRITICAL: Statistical baseline = mean + 2*stdev (95.4% confidence interval)
        baseline_statistical = mean + (2 * stdev)
        
        baselines[key] = {
            "count": len(durations),
            "mean": mean,
            "median": median,
            "stdev": stdev,
            "p95": p95,
            "mad": mad,
            "baseline_statistical": baseline_statistical,  # âœ… PRIMARY THRESHOLD
            "min": float(np.min(durations_array)),
            "max": float(np.max(durations_array))
        }
    
    return baselines

def save_baselines_to_json(baselines: dict, output_path: str):
    """Save baseline statistics to JSON file"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(baselines, f, indent=2)

def load_baselines_from_json(input_path: str) -> dict:
    """Load baseline statistics from JSON file"""
    try:
        with open(input_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        raise FileNotFoundError(f"Baseline file not found: {input_path}")
    except json.JSONDecodeError:
        raise ValueError(f"Invalid JSON in baseline file: {input_path}")

# ============================================================================
# FEATURE ENGINEERING
# ============================================================================

def extract_features_from_baseline(job_name: str, joid: int, 
                                  baselines: Dict[str, Dict],
                                  feature_columns: List[str]) -> Optional[List[float]]:
    """
    Extract features from baseline statistics
    
    Args:
        job_name: Job name
        joid: Job ID
        baselines: Baseline statistics dictionary
        feature_columns: List of feature column names
    
    Returns:
        Feature vector or None if baseline not found
    """
    key = f"{job_name}|{joid}"
    baseline = baselines.get(key)
    
    if not baseline:
        return None
    
    features = []
    for col in feature_columns:
        col_name = col.split("baseline_")[-1]
        value = baseline.get(col_name, baseline.get(col, 0.0))
        features.append(float(value))
    
    return features

# ============================================================================
# MODEL COMPARISON
# ============================================================================

def compare_models(model1_metrics: ModelMetrics, model2_metrics: ModelMetrics) -> Dict:
    """Compare two models based on metrics"""
    comparison = {
        'model1': model1_metrics.to_dict(),
        'model2': model2_metrics.to_dict(),
        'better_model': 'model1' if model1_metrics.rmse < model2_metrics.rmse else 'model2',
        'rmse_difference': abs(model1_metrics.rmse - model2_metrics.rmse),
        'rmse_improvement_percent': (
            (model2_metrics.rmse - model1_metrics.rmse) / model2_metrics.rmse * 100
            if model2_metrics.rmse > 0 else 0
        ),
        'faster_training': (
            'model1' if model1_metrics.training_time_sec < model2_metrics.training_time_sec 
            else 'model2'
        )
    }
    
    return comparison

# ============================================================================
# VALIDATION
# ============================================================================

def validate_features(baselines: dict, feature_columns: List[str]) -> Tuple[bool, List[str]]:
    """
    Validate that all required features exist in baselines
    
    Returns:
        Tuple of (is_valid, list_of_errors)
    """
    errors = []
    
    if not baselines:
        errors.append("Baselines dictionary is empty")
        return False, errors
    
    # Check first baseline entry
    sample_key = list(baselines.keys())[0]
    sample = baselines[sample_key]
    
    for feature in feature_columns:
        col_name = feature.split("baseline_")[-1]
        if col_name not in sample:
            errors.append(f"Missing feature: {col_name}")
    
    return len(errors) == 0, errors

def print_model_metrics(metrics: ModelMetrics, model_name: str = "Model"):
    """Pretty print model metrics"""
    print(f"\n{'='*60}")
    print(f"ðŸ“Š {model_name} Metrics")
    print(f"{'='*60}")
    print(f"RMSE (Root Mean Squared Error):  {metrics.rmse:.2f}s")
    print(f"MAE (Mean Absolute Error):       {metrics.mae:.2f}s")
    print(f"MAPE (Mean Absolute % Error):    {metrics.mape:.2%}")
    print(f"RÂ² Score:                         {metrics.r2:.4f}")
    print(f"Model Size:                       {metrics.model_size_mb:.2f} MB")
    print(f"Training Time:                    {metrics.training_time_sec:.2f}s")
    print(f"{'='*60}\n")

# ============================================================================
# USAGE EXAMPLE
# ============================================================================

if __name__ == "__main__":
    print("âœ… code2.py - Model Training Module (FIXED)")
    print("\nAvailable components:")
    print("  âœ… ModelMetrics - Comprehensive metrics dataclass")
    print("  âœ… compute_metrics() - Calculate all metrics")
    print("  âœ… XGBoostTrainer - Fast XGBoost implementation")
    print("  âœ… LightGBMTrainer - Ultra-fast LightGBM")
    print("  âœ… generate_baseline_statistics() - Mean + 2*StdDev method")
    print("  âœ… FEATURE_COLUMNS - Updated with baseline_stdev and baseline_statistical")


------------------------------------------old livestreamer--------------------

from __future__ import annotations
import argparse
import re
import sys
import time
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Callable, Iterable, Iterator, Optional, Sequence

_TIMESTAMP_RE = re.compile(r"\[(\d{2}/\d{2}/\d{4} \d{2}:\d{2}:\d{2})\]")
_TS_FMT = "%m/%d/%Y %H:%M:%S"


@dataclass(frozen=True)
class LogEntry:
    """Represents one logical Autosys log entry."""

    timestamp: Optional[datetime]
    text: str


def _split_line_into_entries(line: str) -> Sequence[LogEntry]:
    """Split a physical line into logical entries if it embeds multiple timestamps."""

    matches = list(_TIMESTAMP_RE.finditer(line))
    if not matches:
        return (LogEntry(timestamp=None, text=line.strip()),)

    entries: list[LogEntry] = []
    for idx, match in enumerate(matches):
        ts = datetime.strptime(match.group(1), _TS_FMT)
        start = match.start()
        end = matches[idx + 1].start() if idx + 1 < len(matches) else len(line)
        chunk = line[start:end].strip()
        entries.append(LogEntry(timestamp=ts, text=chunk))
    return entries


def _iter_entries(path: Path) -> Iterator[LogEntry]:
    with path.open("r", encoding="utf-8") as handle:
        for raw_line in handle:
            stripped = raw_line.rstrip("\n")
            if not stripped:
                continue
            yield from _split_line_into_entries(stripped)


class AutosysLogStreamer:
    """Streams historic Autosys logs as if they were emitted live."""

    def __init__(
        self,
        log_path: Path,
        *,
        speed: float = 1.0,
        since: Optional[datetime] = None,
        until: Optional[datetime] = None,
        loop: bool = False,
        sleeper: Callable[[float], None] = time.sleep,
    ) -> None:
        if speed <= 0:
            raise ValueError("speed must be > 0")
        self.log_path = log_path
        self.speed = speed
        self.since = since
        self.until = until
        self.loop = loop
        self._sleep = sleeper

    def _should_emit(self, entry: LogEntry) -> bool:
        ts = entry.timestamp
        if ts is None:
            return True
        if self.since and ts < self.since:
            return False
        if self.until and ts > self.until:
            return False
        return True

    def _compute_wait(self, prev_ts: Optional[datetime], current_ts: Optional[datetime]) -> float:
        if prev_ts is None or current_ts is None:
            return 0.0
        delta = (current_ts - prev_ts).total_seconds()
        if delta <= 0:
            return 0.0
        return delta / self.speed

    def stream(self, emit: Callable[[str], None] = None) -> None:
        """Replay the log file honoring the original inter-entry timing."""

        sink = emit or (lambda text: sys.stdout.write(text + "\n"))
        self.current_sim_time = None

        while True:
            previous_ts: Optional[datetime] = None
            emitted_any = False
            for entry in _iter_entries(self.log_path):
                if not self._should_emit(entry):
                    if entry.timestamp and self.until and entry.timestamp > self.until:
                        # No need to continue scanning - future entries will only be later.
                        return
                    continue

                wait_seconds = self._compute_wait(previous_ts, entry.timestamp)
                if wait_seconds:
                    self._sleep(wait_seconds)
                if self.current_sim_time:
                    enriched = f"__SIM_TIME__:{self.current_sim_time.isoformat()}|{entry.text}"    
                    sink(enriched)
                else:
                    sink(entry.text)
                    
                emitted_any = True
                previous_ts = entry.timestamp or previous_ts

            if not self.loop or not emitted_any:
                break


def _parse_dt(value: Optional[str]) -> Optional[datetime]:
    if not value:
        return None
    return datetime.strptime(value, _TS_FMT)

def build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Replay Autosys logs with their original timing offsets.",
    )
    parser.add_argument(
        "--log-file",
        default="D:\AUTOSYSLOG\event_demon.SBI.09012025.txt",
        type=Path,
    )
    parser.add_argument(
        "--speed",
        default=1.0,
        type=float,
        help="Speed multiplier relative to real time (e.g. 2.0 = twice as fast).",
    )
    parser.add_argument(
        "--since",
        type=str,
        help="Optional lower bound timestamp in MM/DD/YYYY HH:MM:SS.",
    )
    parser.add_argument(
        "--until",
        type=str,
        help="Optional upper bound timestamp in MM/DD/YYYY HH:MM:SS.",
    )
    parser.add_argument(
        "--loop",
        action="store_true",
        help="Loop back to the start once the end of the file is reached.",
    )
    return parser

def main(argv: Optional[Iterable[str]] = None):
    parser = build_arg_parser()
    args = parser.parse_args(argv)

    streamer = AutosysLogStreamer(
        log_path=args.log_file,
        speed=args.speed,
        since=_parse_dt(args.since),
        until=_parse_dt(args.until),
        loop=args.loop,
    )
    streamer.stream()

if __name__ == "__main__":
    main()

--------------------------------old prepr----------------------


import re
from datetime import datetime

# Regex patterns
TS_RE = re.compile(r"\[(?P<ts>\d{2}/\d{2}/\d{4} \d{2}:\d{2}:\d{2})\]")
STATUS_RE = re.compile(r"\bSTATUS:\s*(?P<status>\w+)\b", re.IGNORECASE)
JOB_RE = re.compile(
    r"\bJOB:\s*(?P<job>.+?)(?=\s+(?:MACHINE:|JOID:|EXITCODE:|RUNID:|NTRY:|\Z))",
    re.IGNORECASE,
)
MACHINE_RE = re.compile(r"\bMACHINE:\s*(?P<machine>[^\s:]+)", re.IGNORECASE)
JOID_RE = re.compile(r"\bJOID:\s*(?P<joid>\d+)\b", re.IGNORECASE)
RUNID_RE = re.compile(r"\bRUNID:\s*(?P<runid>\d+)\b", re.IGNORECASE)


def parse_ts(ts_str):
    """Parse timestamp from AutoSys log format"""
    try:
        return datetime.strptime(ts_str, "%m/%d/%Y %H:%M:%S")
    except:
        return None


# State holders
first_event = {}
starting_done = set()


def preprocess_stream_line(line: str):
    
    if len(line.strip()) < 3:
        return None

    ts_m = TS_RE.search(line)
    status_m = STATUS_RE.search(line)
    job_m = JOB_RE.search(line)
    machine_m = MACHINE_RE.search(line)
    joid_m = JOID_RE.search(line)
    runid_m = RUNID_RE.search(line)

    # âœ… CRITICAL: Extract timestamp from log (the "ts" field)
    ts = parse_ts(ts_m.group("ts")) if ts_m else None
    status = status_m.group("status").upper() if status_m else None
    job = job_m.group("job").strip() if job_m else None
    machine = machine_m.group("machine").strip() if machine_m else ""
    joid = int(joid_m.group("joid")) if joid_m else None
    runid = int(runid_m.group("runid")) if runid_m else None

    rec = {
        "JOID": joid,
        "JobName": job,
        "Machine": machine,
        "RUNID": runid,
        "Status": status,
        "timestamp": ts,  # âœ… CRITICAL: This is the log timestamp, NOT system time
    }
    
    key = (joid, runid)
    
    # Only consider valid Autosys lifecycle events
    if status not in ("STARTING", "RUNNING", "SUCCESS", "FAILURE"):
        return None
    
    status_clean = status
    
    # If JOID missing â†’ cannot reconstruct lifecycle â†’ return as-is
    if key is None or joid is None:
        return rec
    
    # First event handling
    if key not in first_event:
        first_event[key] = status_clean
        
        if status_clean == "STARTING":
            starting_done.add(key)
            return rec
        
        if status_clean == "RUNNING":
            if key not in starting_done:
                # Inject fake STARTING with same timestamp
                fake = dict(rec)
                fake["Status"] = "STARTING"
                starting_done.add(key)
                return fake
            return rec
        
        # âœ… FIXED: Always return FAILURE events
        if status_clean == "FAILURE":
            return rec
        
        return rec
    
    # Prevent duplicate STARTING
    if status_clean == "STARTING" and key in starting_done:
        return None
    
    if status_clean == "STARTING":
        starting_done.add(key)
        return rec
    
    # âœ… CRITICAL: Always return SUCCESS and FAILURE events
    if status_clean in ("SUCCESS", "FAILURE"):
        return rec
    
    return rec

=============================================================
===============================================================
====================================================================
======================preproce------------------------

import re
from datetime import datetime

# Regex patterns
TS_RE = re.compile(r"\[(?P<ts>\d{2}/\d{2}/\d{4} \d{2}:\d{2}:\d{2})\]")
STATUS_RE = re.compile(r"\bSTATUS:\s*(?P<status>\w+)\b", re.IGNORECASE)
JOB_RE = re.compile(
    r"\bJOB:\s*(?P<job>.+?)(?=\s+(?:MACHINE:|JOID:|EXITCODE:|RUNID:|NTRY:|\Z))",
    re.IGNORECASE,
)
MACHINE_RE = re.compile(r"\bMACHINE:\s*(?P<machine>[^\s:]+)", re.IGNORECASE)
JOID_RE = re.compile(r"\bJOID:\s*(?P<joid>\d+)\b", re.IGNORECASE)
RUNID_RE = re.compile(r"\bRUNID:\s*(?P<runid>\d+)\b", re.IGNORECASE)


def parse_ts(ts_str):
    """Parse timestamp from AutoSys log format"""
    try:
        return datetime.strptime(ts_str, "%m/%d/%Y %H:%M:%S")
    except:
        return None


# State holders
first_event = {}
starting_done = set()


def preprocess_stream_line(line: str):
    
    if len(line.strip()) < 3:
        return None

    ts_m = TS_RE.search(line)
    status_m = STATUS_RE.search(line)
    job_m = JOB_RE.search(line)
    machine_m = MACHINE_RE.search(line)
    joid_m = JOID_RE.search(line)
    runid_m = RUNID_RE.search(line)

    # âœ… CRITICAL: Extract timestamp from log (the "ts" field)
    ts = parse_ts(ts_m.group("ts")) if ts_m else None
    status = status_m.group("status").upper() if status_m else None
    job = job_m.group("job").strip() if job_m else None
    machine = machine_m.group("machine").strip() if machine_m else ""
    joid = int(joid_m.group("joid")) if joid_m else None
    runid = int(runid_m.group("runid")) if runid_m else None

    rec = {
        "JOID": joid,
        "JobName": job,
        "Machine": machine,
        "RUNID": runid,
        "Status": status,
        "timestamp": ts,  # âœ… CRITICAL: This is the log timestamp, NOT system time
    }
    
    key = (joid, runid)
    
    # Only consider valid Autosys lifecycle events
    if status not in ("STARTING", "RUNNING", "SUCCESS", "FAILURE"):
        return None
    
    status_clean = status
    
    # If JOID missing â†’ cannot reconstruct lifecycle â†’ return as-is
    if key is None or joid is None:
        return rec
    
    # First event handling
    if key not in first_event:
        first_event[key] = status_clean
        
        if status_clean == "STARTING":
            starting_done.add(key)
            return rec
        
        if status_clean == "RUNNING":
            if key not in starting_done:
                # Inject fake STARTING with same timestamp
                fake = dict(rec)
                fake["Status"] = "STARTING"
                starting_done.add(key)
                return fake
            return rec
        
        # âœ… FIXED: Always return FAILURE events
        if status_clean == "FAILURE":
            return rec
        
        return rec
    
    # Prevent duplicate STARTING
    if status_clean == "STARTING" and key in starting_done:
        return None
    
    if status_clean == "STARTING":
        starting_done.add(key)
        return rec
    
    # âœ… CRITICAL: Always return SUCCESS and FAILURE events
    if status_clean in ("SUCCESS", "FAILURE"):
        return rec
    
    return rec

==================================auito

from __future__ import annotations
import argparse
import re
import sys
import time
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Callable, Iterable, Iterator, Optional, Sequence

_TIMESTAMP_RE = re.compile(r"\[(\d{2}/\d{2}/\d{4} \d{2}:\d{2}:\d{2})\]")
_TS_FMT = "%m/%d/%Y %H:%M:%S"


@dataclass(frozen=True)
class LogEntry:
    """Represents one logical Autosys log entry."""

    timestamp: Optional[datetime]
    text: str


def _split_line_into_entries(line: str) -> Sequence[LogEntry]:
    """Split a physical line into logical entries if it embeds multiple timestamps."""

    matches = list(_TIMESTAMP_RE.finditer(line))
    if not matches:
        return (LogEntry(timestamp=None, text=line.strip()),)

    entries: list[LogEntry] = []
    for idx, match in enumerate(matches):
        ts = datetime.strptime(match.group(1), _TS_FMT)
        start = match.start()
        end = matches[idx + 1].start() if idx + 1 < len(matches) else len(line)
        chunk = line[start:end].strip()
        entries.append(LogEntry(timestamp=ts, text=chunk))
    return entries


def _iter_entries(path: Path) -> Iterator[LogEntry]:
    with path.open("r", encoding="utf-8") as handle:
        for raw_line in handle:
            stripped = raw_line.rstrip("\n")
            if not stripped:
                continue
            yield from _split_line_into_entries(stripped)


class AutosysLogStreamer:
    """Streams historic Autosys logs as if they were emitted live."""

    def __init__(
        self,
        log_path: Path,
        *,
        speed: float = 1.0,
        since: Optional[datetime] = None,
        until: Optional[datetime] = None,
        loop: bool = False,
        sleeper: Callable[[float], None] = time.sleep,
    ) -> None:
        if speed <= 0:
            raise ValueError("speed must be > 0")
        self.log_path = log_path
        self.speed = speed
        self.since = since
        self.until = until
        self.loop = loop
        self._sleep = sleeper

    def _should_emit(self, entry: LogEntry) -> bool:
        ts = entry.timestamp
        if ts is None:
            return True
        if self.since and ts < self.since:
            return False
        if self.until and ts > self.until:
            return False
        return True

    def _compute_wait(self, prev_ts: Optional[datetime], current_ts: Optional[datetime]) -> float:
        if prev_ts is None or current_ts is None:
            return 0.0
        delta = (current_ts - prev_ts).total_seconds()
        if delta <= 0:
            return 0.0
        return delta / self.speed

    def stream(self, emit):

        entries = list(_iter_entries(self.log_path))
        if not entries:
            return

        # 1ï¸âƒ£ Find first log timestamp
        first_ts = None
        for e in entries:
            if e.timestamp:
                first_ts = e.timestamp
                break

        if first_ts is None:
            # No timestamps â†’ just emit
            for e in entries:
                emit(e.text)
            return

        # 2ï¸âƒ£ Capture system start time
        wall_start = time.time()  # real wall clock seconds

        # 3ï¸âƒ£ Replay with PERFECT time sync
        for entry in entries:

            if not self._should_emit(entry):
                continue

            if entry.timestamp:
                # Autosys elapsed time
                auto_offset = (entry.timestamp - first_ts).total_seconds()

                # Target system clock moment
                target_wall = wall_start + auto_offset

                # Current system time
                now = time.time()
                wait = target_wall - now

                if wait > 0:
                    time.sleep(wait)

            # Emit the actual log line
            emit(entry.text)


def _parse_dt(value: Optional[str]) -> Optional[datetime]:
    if not value:
        return None
    return datetime.strptime(value, _TS_FMT)

def build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Replay Autosys logs with their original timing offsets.",
    )
    parser.add_argument(
        "--log-file",
        default="D:\AUTOSYSLOG\event_demon.SBI.09012025.txt",
        type=Path,
    )
    parser.add_argument(
        "--speed",
        default=1.0,
        type=float,
        help="Speed multiplier relative to real time (e.g. 2.0 = twice as fast).",
    )
    parser.add_argument(
        "--since",
        type=str,
        help="Optional lower bound timestamp in MM/DD/YYYY HH:MM:SS.",
    )
    parser.add_argument(
        "--until",
        type=str,
        help="Optional upper bound timestamp in MM/DD/YYYY HH:MM:SS.",
    )
    parser.add_argument(
        "--loop",
        action="store_true",
        help="Loop back to the start once the end of the file is reached.",
    )
    return parser

def main(argv: Optional[Iterable[str]] = None):
    parser = build_arg_parser()
    args = parser.parse_args(argv)

    streamer = AutosysLogStreamer(
        log_path=args.log_file,
        speed=args.speed,
        since=_parse_dt(args.since),
        until=_parse_dt(args.until),
        loop=args.loop,
    )
    streamer.stream()

if __name__ == "__main__":
    main()

===========================runner---------------

# runner.py
from __future__ import annotations

import argparse
import time
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, Iterable, Callable

from live_streamer import AutosysLogStreamer, _iter_entries
from preprocessing import preprocess_stream_line


def make_txt_emitter(out_path: Path) -> Callable[[str], None]:
    out_path.parent.mkdir(parents=True, exist_ok=True)
    f = out_path.open("a", encoding="utf-8")

    def emit(line: str) -> None:
        sys_ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        raw = line.rstrip("\n")
        f.write(f"[SYS {sys_ts}] {raw}\n")
        f.flush()

    return emit


def stream_with_clock_sync(
    streamer: AutosysLogStreamer,
    emit: Callable[[str], None],
) -> None:
    log_path = streamer.log_path
    speed = streamer.speed

    # Read all entries once (file is finite historic log)
    entries = list(_iter_entries(log_path))
    if not entries:
        return

    # Find first entry that has a timestamp
    first_ts: Optional[datetime] = None
    for e in entries:
        if e.timestamp is not None:
            first_ts = e.timestamp
            break

    if first_ts is None:
        # No timestamps in file â†’ just emit without timing control
        for e in entries:
            if streamer._should_emit(e):
                emit(e.text)
        return

    start_wall = datetime.now()

    for entry in entries:
        if not streamer._should_emit(entry):
            continue

        if entry.timestamp is not None:
            # Offset in original Autosys time
            delta = (entry.timestamp - first_ts).total_seconds()
            # Apply speed factor
            logical_offset = delta / speed
            target_wall = start_wall + timedelta(seconds=logical_offset)

            now = datetime.now()
            wait = (target_wall - now).total_seconds()
            if wait > 0:
                time.sleep(wait)

        # At this point we are as close as possible to the target wall-clock time
        emit(entry.text)


def build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Replay Autosys logs, clock-synced to system time, and store output in txt file."
    )
    parser.add_argument(
        "--log-file",
        type=Path,
        default=Path(r"D:\AUTOSYSLOG\test\event_demon.SBI.09012025.txt"),
        help="Input Autosys log file (historic).",
    )
    parser.add_argument(
        "--out-file",
        type=Path,
        default=Path(r"D:\AI SBI\MonitAI_2\logs\replayed_with_system_time.txt"),
        help="Output txt file with system timestamps.",
    )
    parser.add_argument(
        "--speed",
        type=float,
        default=1.0,
        help="Speed multiplier (1.0 = real time, 2.0 = twice as fast, 0.5 = half speed).",
    )
    return parser


def _parse_dt(value: Optional[str]) -> Optional[datetime]:
    if not value:
        return None
    return datetime.strptime(value, "%m/%d/%Y %H:%M:%S")


def main(argv: Optional[Iterable[str]] = None) -> None:
    parser = build_arg_parser()
    args = parser.parse_args(argv)

    streamer = AutosysLogStreamer(
        log_path=args.log_file,
        speed=args.speed,
    )

    emit = make_txt_emitter(args.out_file)
    stream_with_clock_sync(streamer, emit)


if __name__ == "__main__":
    main()